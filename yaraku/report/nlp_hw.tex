
\documentclass{kthreport}
% default language is English, but you can use Swedish definitions like this:
% \documentclass[swedish]{kthreport}

% Remember that in order for the class to find the KTH logo, you need to
% have it in your path, for example:
% export TEXINPUTS=/path/to/logo/location//:$TEXINPUTS

% Packages
\usepackage{hyperref}
% Packages

% Env
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\title{NLP assignment from yaraku}
% \subtitle{This will be read by all employees}
\author{Chun Hung Lin, Chris}
\diarienr{00000-00}

\begin{document}
\maketitle

% Here is the first paragraph. You can describe what the report is all about.
% Then you'd better start some sections.

\section{Question 1}

Download a list of English words from the corpus at \hspace{0.1em}
\href{https://github.com/dwyl/english-words}{github.com/dwyl/english-words}.
(download words\_alpha.zip) and cluster a random sample of 10000 of
these words into different clusters using a suitable method.

Implement the code in a Jupyter Notebook, display the results and visualisation
of the clustering in the notebook and export the Notebook into an HTML file.

Submit the HTML file.

\section{Question 2}
\subsection{Question 2a}
In the previous exercise, you clustered words.
Describe how you might cluster multi-word phrases or sentences or paragraphs of
arbitrary lengths.

\textbf{Ans:}
xxxxxxxxxxxxxxx

\subsection{Question 2b (Optional)}
Describe how you might cluster words from different languages

\textbf{Ans:}
xxxxxxxxxxxxxxx
% -----------------------------------------------------------------
\section{Question 3}
The earliest deep-learning attention mechanism was the method proposed by Bahdanau
in a paper on sequence to sequence models.

Name some other variants of attention mechanisms (other than Bahdanau's method).

\subsection*{Ans:}

\textbf{Bahdanau mechanism}


Bahdanau purposed the attention model which is to solve the bottleneck of using 
fixed-length vector in the basic encoder-decoder architecture. It is obvious to understand
that encoding a long sentence into a fixed-length context vector (representation) 
will lose the information like position information (i.e. How the word orders in the sentence)
and alignment between source and target sentences.

\textbf{Luong mechanism}

In this paper \cite{luong-etal-2015-effective}, they purposed 3 other method to calculate the alignment weightings
and make modifcation to the Bahdanau's method. The modifcations are 
\begin{itemize}
    \item use the hidden state at the top LSTM or RNN-based network rs in both the encoder and decoder
    \item use the current decoder state to compute the alignment weightings
\end{itemize}

Also, Luong et al work \cite{luong-etal-2015-effective} purposed global and local attentions.

\textbf{Self Attention}

Self-attention was purposed by Cheng et. al. \cite{cheng-etal-2016-long}. It is 
original to create a facility to give the RNN-famaily models stronger memorization 
capability and the ability to discover relations among tokens.

\textbf{Scaled Dot-Product and Multi-Head Self Attention}

It is the core component of the transformer model. \cite{vaswani2017attention}
Multi-head attention makes the model have different representations of 
learnable subspaces for keys, values and queries inputs of each layer.


% Good starting points: 
% 1. ruder.io/deep-learning-nlp-best-practices/index.html#attention
% 2. towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#7eef

% Ref:
% 1. lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#summary

\section{Question 4}
\subsection{Question 4a}
Come up with an architecture for an ANN that would suffice to add up the digits of a 4-digit number.

\subsection{Question 4b}
Try to find the smallest that can do the job.
What is the minimum number of fully connected layers (separated by non-linearities)
that an ANN must have to compute the XOR function.

\section{Question 5}
How can you tell if an ML model is underfitting and overfitting?

% ---------------------------------------------------------------------
\medskip

\bibliographystyle{unsrt}
\bibliography{ref}
\end{document}
